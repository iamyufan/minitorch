{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4pWhxZCyMQ-"
      },
      "source": [
        "# How to run on GPU\n",
        "\n",
        "Step 1) Go to https://github.com/settings/tokens and get a token that can read your private repos\n",
        "\n",
        "Step 2) Clone this colab notebook and change the execution environment to GPU.\n",
        "\n",
        "Step 3) Install python 3.8\n",
        "\n",
        "Step 4) Clone and install your code.\n",
        "\n",
        "Step 5) Run the command-line code for training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTzMYZB5yynZ"
      },
      "source": [
        "## Step 3: Install python 3.8\n",
        "Run the cell below without editing it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lciKNd6myIjl",
        "outputId": "4c8ba607-883a-4c0f-9ada-017d72991c30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [631 kB]\n",
            "Hit:5 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,013 kB]\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.0 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,240 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,456 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,512 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,281 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [49.8 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,487 kB]\n",
            "Fetched 8,949 kB in 3s (3,080 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpython3.8-minimal libpython3.8-stdlib mailcap mime-support\n",
            "  python3.8-minimal\n",
            "Suggested packages:\n",
            "  python3.8-venv binfmt-support\n",
            "The following NEW packages will be installed:\n",
            "  libpython3.8-minimal libpython3.8-stdlib mailcap mime-support python3.8\n",
            "  python3.8-minimal\n",
            "0 upgraded, 6 newly installed, 0 to remove and 15 not upgraded.\n",
            "Need to get 5,098 kB of archives.\n",
            "After this operation, 18.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 mailcap all 3.70+nmu1ubuntu1 [23.8 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 mime-support all 3.66 [3,696 B]\n",
            "Get:3 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.8-minimal amd64 3.8.18-1+jammy1 [794 kB]\n",
            "Get:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-minimal amd64 3.8.18-1+jammy1 [2,024 kB]\n",
            "Get:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.8-stdlib amd64 3.8.18-1+jammy1 [1,815 kB]\n",
            "Get:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8 amd64 3.8.18-1+jammy1 [438 kB]\n",
            "Fetched 5,098 kB in 6s (841 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 6.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libpython3.8-minimal:amd64.\n",
            "(Reading database ... 120880 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libpython3.8-minimal_3.8.18-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.8-minimal:amd64 (3.8.18-1+jammy1) ...\n",
            "Selecting previously unselected package python3.8-minimal.\n",
            "Preparing to unpack .../1-python3.8-minimal_3.8.18-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.8-minimal (3.8.18-1+jammy1) ...\n",
            "Selecting previously unselected package mailcap.\n",
            "Preparing to unpack .../2-mailcap_3.70+nmu1ubuntu1_all.deb ...\n",
            "Unpacking mailcap (3.70+nmu1ubuntu1) ...\n",
            "Selecting previously unselected package mime-support.\n",
            "Preparing to unpack .../3-mime-support_3.66_all.deb ...\n",
            "Unpacking mime-support (3.66) ...\n",
            "Selecting previously unselected package libpython3.8-stdlib:amd64.\n",
            "Preparing to unpack .../4-libpython3.8-stdlib_3.8.18-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.8-stdlib:amd64 (3.8.18-1+jammy1) ...\n",
            "Selecting previously unselected package python3.8.\n",
            "Preparing to unpack .../5-python3.8_3.8.18-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.8 (3.8.18-1+jammy1) ...\n",
            "Setting up libpython3.8-minimal:amd64 (3.8.18-1+jammy1) ...\n",
            "Setting up python3.8-minimal (3.8.18-1+jammy1) ...\n",
            "Setting up mailcap (3.70+nmu1ubuntu1) ...\n",
            "Setting up mime-support (3.66) ...\n",
            "Setting up libpython3.8-stdlib:amd64 (3.8.18-1+jammy1) ...\n",
            "Setting up python3.8 (3.8.18-1+jammy1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2570k  100 2570k    0     0  6370k      0 --:--:-- --:--:-- --:--:-- 6362k\n",
            "Collecting pip\n",
            "  Downloading pip-23.3.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Downloading pip-23.3.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-23.3.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!sudo apt-get update -y\n",
        "!sudo apt-get install python3.8\n",
        "!curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n",
        "!python3 get-pip.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guhMlPoey3zE"
      },
      "source": [
        "## Step 4: Clone and install your code\n",
        "First we need to set some environment variables. Get your github API token and MLE repo username and put them into the below variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmmAQK12zJJP",
        "outputId": "3bbb5469-4779-48d4-f3a4-f17ace7cc7a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: TOKEN=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\n",
            "env: USER=iamyufan\n"
          ]
        }
      ],
      "source": [
        "%env TOKEN=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\n",
        "%env USER=iamyufan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JZt2xZwzS2b"
      },
      "source": [
        "Run the below code. Editing should not be necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYb6KtYFzJjD",
        "outputId": "bfdb967c-79c9-4635-f0fe-67a9c2f55f1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: DIR=mle-module-3-iamyufan\n",
            "https://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@github.com/Cornell-Tech-ML/mle-module-3-iamyufan\n",
            "Cloning into 'mle-module-3-iamyufan'...\n",
            "remote: Enumerating objects: 206, done.\u001b[K\n",
            "remote: Counting objects: 100% (206/206), done.\u001b[K\n",
            "remote: Compressing objects: 100% (143/143), done.\u001b[K\n",
            "remote: Total 206 (delta 114), reused 132 (delta 59), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (206/206), 107.97 KiB | 1.96 MiB/s, done.\n",
            "Resolving deltas: 100% (114/114), done.\n",
            "Collecting altair==4.0 (from -r requirements.txt (line 1))\n",
            "  Downloading altair-4.0.0-py2.py3-none-any.whl (709 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m709.0/709.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorama==0.4.3 (from -r requirements.txt (line 2))\n",
            "  Downloading colorama-0.4.3-py2.py3-none-any.whl (15 kB)\n",
            "Collecting hypothesis==6.54 (from -r requirements.txt (line 3))\n",
            "  Downloading hypothesis-6.54.0-py3-none-any.whl (389 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mypy==0.981 (from -r requirements.txt (line 4))\n",
            "  Downloading mypy-0.981-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.7/17.7 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numba==0.58 (from -r requirements.txt (line 5))\n",
            "  Downloading numba-0.58.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
            "Collecting numpy==1.24.1 (from -r requirements.txt (line 6))\n",
            "  Downloading numpy-1.24.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pre-commit==2.20.0 (from -r requirements.txt (line 7))\n",
            "  Downloading pre_commit-2.20.0-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.5/199.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytest==7.1.2 (from -r requirements.txt (line 8))\n",
            "  Downloading pytest-7.1.2-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.0/297.0 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytest-env (from -r requirements.txt (line 9))\n",
            "  Downloading pytest_env-1.1.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting pytest-runner==5.2 (from -r requirements.txt (line 10))\n",
            "  Downloading pytest_runner-5.2-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (4.5.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair==4.0->-r requirements.txt (line 1)) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair==4.0->-r requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from altair==4.0->-r requirements.txt (line 1)) (4.19.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from altair==4.0->-r requirements.txt (line 1)) (1.5.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair==4.0->-r requirements.txt (line 1)) (0.12.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from hypothesis==6.54->-r requirements.txt (line 3)) (23.1.0)\n",
            "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from hypothesis==6.54->-r requirements.txt (line 3)) (2.4.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from hypothesis==6.54->-r requirements.txt (line 3)) (1.1.3)\n",
            "Collecting mypy-extensions>=0.4.3 (from mypy==0.981->-r requirements.txt (line 4))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from mypy==0.981->-r requirements.txt (line 4)) (2.0.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba==0.58->-r requirements.txt (line 5)) (0.41.1)\n",
            "Collecting cfgv>=2.0.0 (from pre-commit==2.20.0->-r requirements.txt (line 7))\n",
            "  Downloading cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting identify>=1.0.0 (from pre-commit==2.20.0->-r requirements.txt (line 7))\n",
            "  Downloading identify-2.5.32-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting nodeenv>=0.11.1 (from pre-commit==2.20.0->-r requirements.txt (line 7))\n",
            "  Downloading nodeenv-1.8.0-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit==2.20.0->-r requirements.txt (line 7)) (6.0.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pre-commit==2.20.0->-r requirements.txt (line 7)) (0.10.2)\n",
            "Collecting virtualenv>=20.0.8 (from pre-commit==2.20.0->-r requirements.txt (line 7))\n",
            "  Downloading virtualenv-20.24.6-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest==7.1.2->-r requirements.txt (line 8)) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest==7.1.2->-r requirements.txt (line 8)) (23.2)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest==7.1.2->-r requirements.txt (line 8)) (1.3.0)\n",
            "Collecting py>=1.8.2 (from pytest==7.1.2->-r requirements.txt (line 8))\n",
            "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of pytest-env to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pytest-env (from -r requirements.txt (line 9))\n",
            "  Downloading pytest_env-1.1.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "  Downloading pytest_env-1.0.1-py3-none-any.whl.metadata (4.5 kB)\n",
            "  Downloading pytest_env-1.0.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "  Downloading pytest_env-0.8.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "  Downloading pytest_env-0.8.1-py3-none-any.whl (5.2 kB)\n",
            "  Downloading pytest_env-0.8.0-py3-none-any.whl (5.2 kB)\n",
            "  Downloading pytest_env-0.7.0-py3-none-any.whl (4.9 kB)\n",
            "INFO: pip is still looking at multiple versions of pytest-env to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading pytest-env-0.6.2.tar.gz (1.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nodeenv>=0.11.1->pre-commit==2.20.0->-r requirements.txt (line 7)) (67.7.2)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.0.8->pre-commit==2.20.0->-r requirements.txt (line 7))\n",
            "  Downloading distlib-0.3.7-py2.py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.0.8->pre-commit==2.20.0->-r requirements.txt (line 7)) (3.13.1)\n",
            "Collecting platformdirs<4,>=3.9.1 (from virtualenv>=20.0.8->pre-commit==2.20.0->-r requirements.txt (line 7))\n",
            "  Downloading platformdirs-3.11.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair==4.0->-r requirements.txt (line 1)) (2.1.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->altair==4.0->-r requirements.txt (line 1)) (2023.11.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->altair==4.0->-r requirements.txt (line 1)) (0.31.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->altair==4.0->-r requirements.txt (line 1)) (0.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->altair==4.0->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->altair==4.0->-r requirements.txt (line 1)) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->altair==4.0->-r requirements.txt (line 1)) (1.16.0)\n",
            "Downloading numba-0.58.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\n",
            "Downloading identify-2.5.32-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.9/98.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nodeenv-1.8.0-py2.py3-none-any.whl (22 kB)\n",
            "Downloading virtualenv-20.24.6-py3-none-any.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m103.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.3.7-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading platformdirs-3.11.0-py3-none-any.whl (17 kB)\n",
            "Building wheels for collected packages: pytest-env\n",
            "  Building wheel for pytest-env (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytest-env: filename=pytest_env-0.6.2-py3-none-any.whl size=2346 sha256=6cdd190dca264bae80a50ea49d864d49c1ca2dad53d5e00eccb3d867ab26466c\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/dc/5a/862133b6d4688cfc367733985063f559f3b37ba99e62098458\n",
            "Successfully built pytest-env\n",
            "Installing collected packages: distlib, pytest-runner, py, platformdirs, numpy, nodeenv, mypy-extensions, identify, hypothesis, colorama, cfgv, virtualenv, pytest, numba, mypy, pytest-env, pre-commit, altair\n",
            "  Attempting uninstall: platformdirs\n",
            "    Found existing installation: platformdirs 4.0.0\n",
            "    Uninstalling platformdirs-4.0.0:\n",
            "      Successfully uninstalled platformdirs-4.0.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 7.4.3\n",
            "    Uninstalling pytest-7.4.3:\n",
            "      Successfully uninstalled pytest-7.4.3\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.58.1\n",
            "    Uninstalling numba-0.58.1:\n",
            "      Successfully uninstalled numba-0.58.1\n",
            "  Attempting uninstall: altair\n",
            "    Found existing installation: altair 4.2.2\n",
            "    Uninstalling altair-4.2.2:\n",
            "      Successfully uninstalled altair-4.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed altair-4.0.0 cfgv-3.4.0 colorama-0.4.3 distlib-0.3.7 hypothesis-6.54.0 identify-2.5.32 mypy-0.981 mypy-extensions-1.0.0 nodeenv-1.8.0 numba-0.58.0 numpy-1.24.1 platformdirs-3.11.0 pre-commit-2.20.0 py-1.11.0 pytest-7.1.2 pytest-env-0.6.2 pytest-runner-5.2 virtualenv-20.24.6\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m/bin/bash: line 1: pip3.8: command not found\n",
            "Processing /content/mle-module-3-iamyufan\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: minitorch\n",
            "  Building wheel for minitorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for minitorch: filename=minitorch-0.3.0-py3-none-any.whl size=29975 sha256=8c7b769cdfe1bda98627fdb451dbab7eb6e42488d1ce7fa787352a10f390db64\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/cf/a6/654bbf5186b9ad391ca08b3b4467d142f168ab07cf6f3dfad6\n",
            "Successfully built minitorch\n",
            "Installing collected packages: minitorch\n",
            "Successfully installed minitorch-0.3.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "TOKEN = %env TOKEN\n",
        "USER = %env USER\n",
        "%env DIR=mle-module-3-$USER\n",
        "DIR = %env DIR\n",
        "\n",
        "!echo https://$TOKEN@github.com/Cornell-Tech-ML/$DIR\n",
        "\n",
        "!git clone -b master --single-branch https://$TOKEN@github.com/Cornell-Tech-ML/$DIR\n",
        "!cd $DIR; pip3 install -r requirements.txt; pip3.8 install -r requirements.extra.txt; pip3 install ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2xn5AhUzaW_"
      },
      "source": [
        "If you update your code, you can re-pull the repo by running this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GywpmTnozf26",
        "outputId": "562e1c34-4692-4776-cfd2-bd359533c5d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From https://github.com/Cornell-Tech-ML/mle-module-3-iamyufan\n",
            " * branch            master     -> FETCH_HEAD\n",
            "Already up to date.\n",
            "Processing /content/mle-module-3-iamyufan\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: minitorch\n",
            "  Building wheel for minitorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for minitorch: filename=minitorch-0.3.0-py3-none-any.whl size=29975 sha256=6d3cf2f5e831c47f21dc2fb4f836fc4c8c9c59ecac9443a0184229b6d1ec8108\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-f42dlezt/wheels/6c/cf/a6/654bbf5186b9ad391ca08b3b4467d142f168ab07cf6f3dfad6\n",
            "Successfully built minitorch\n",
            "Installing collected packages: minitorch\n",
            "  Attempting uninstall: minitorch\n",
            "    Found existing installation: minitorch 0.3.0\n",
            "    Uninstalling minitorch-0.3.0:\n",
            "      Successfully uninstalled minitorch-0.3.0\n",
            "Successfully installed minitorch-0.3.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!cd $DIR; git pull origin master; pip3 install --force-reinstall --no-cache-dir ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qufLzTGmzs1K"
      },
      "source": [
        "## Step 5: Run the training command"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd $DIR; PYTHONPATH=/content/$DIR pytest tests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3Pf4D_OnDkv",
        "outputId": "dae0229f-3a6d-4a67-d209-40bd12a26e9d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.1.2, pluggy-1.3.0\n",
            "rootdir: /content/mle-module-3-iamyufan, configfile: setup.cfg\n",
            "plugins: hypothesis-6.54.0, env-0.6.2, anyio-3.7.1\n",
            "collected 117 items                                                                                \u001b[0m\n",
            "\n",
            "tests/test_tensor_general.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33m [ 53%]\n",
            "\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33m                                       [100%]\u001b[0m\n",
            "\n",
            "\u001b[33m========================================= warnings summary =========================================\u001b[0m\n",
            "tests/test_tensor_general.py: 20 warnings\n",
            "  /usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "    warn(NumbaPerformanceWarning(msg))\n",
            "\n",
            "tests/test_tensor_general.py: 4377 warnings\n",
            "  /usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "    warn(NumbaPerformanceWarning(msg))\n",
            "\n",
            "tests/test_tensor_general.py: 14 warnings\n",
            "  /usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "    warn(NumbaPerformanceWarning(msg))\n",
            "\n",
            "tests/test_tensor_general.py::test_one_args[cuda-fn10]\n",
            "tests/test_tensor_general.py::test_one_args[cuda-fn12]\n",
            "tests/test_tensor_general.py::test_one_derivative[cuda-fn0]\n",
            "tests/test_tensor_general.py::test_one_derivative[cuda-fn4]\n",
            "tests/test_tensor_general.py::test_sum_practice2\n",
            "tests/test_tensor_general.py::test_bmm[cuda]\n",
            "  /usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 3 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "    warn(NumbaPerformanceWarning(msg))\n",
            "\n",
            "tests/test_tensor_general.py::test_one_derivative[cuda-fn0]\n",
            "tests/test_tensor_general.py::test_bmm[cuda]\n",
            "  /usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 6 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "    warn(NumbaPerformanceWarning(msg))\n",
            "\n",
            "tests/test_tensor_general.py::test_one_derivative[cuda-fn0]\n",
            "  /usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 9 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "    warn(NumbaPerformanceWarning(msg))\n",
            "\n",
            "tests/test_tensor_general.py::test_one_derivative[cuda-fn0]\n",
            "  /usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 18 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "    warn(NumbaPerformanceWarning(msg))\n",
            "\n",
            "tests/test_tensor_general.py::test_one_derivative[cuda-fn0]\n",
            "tests/test_tensor_general.py::test_mul_practice4\n",
            "tests/test_tensor_general.py::test_bmm[cuda]\n",
            "tests/test_tensor_general.py::test_bmm[cuda]\n",
            "  /usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "    warn(NumbaPerformanceWarning(msg))\n",
            "\n",
            "tests/test_tensor_general.py::test_one_derivative[cuda-fn0]\n",
            "tests/test_tensor_general.py::test_mul_practice5\n",
            "  /usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 8 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "    warn(NumbaPerformanceWarning(msg))\n",
            "\n",
            "tests/test_tensor_general.py::test_one_derivative[cuda-fn3]\n",
            "  /usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 12 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "    warn(NumbaPerformanceWarning(msg))\n",
            "\n",
            "tests/test_tensor_general.py::test_one_derivative[cuda-fn3]\n",
            "  /usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 27 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "    warn(NumbaPerformanceWarning(msg))\n",
            "\n",
            "tests/test_tensor_general.py::test_sum_practice_other_dims\n",
            "  /usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 16 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "    warn(NumbaPerformanceWarning(msg))\n",
            "\n",
            "tests/test_tensor_general.py::test_mul_practice4\n",
            "  /usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 35 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "    warn(NumbaPerformanceWarning(msg))\n",
            "\n",
            "tests/test_tensor_general.py::test_bmm[cuda]\n",
            "  /usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 32 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "    warn(NumbaPerformanceWarning(msg))\n",
            "\n",
            "tests/test_tensor_general.py::test_bmm[cuda]\n",
            "  /usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 48 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "    warn(NumbaPerformanceWarning(msg))\n",
            "\n",
            "tests/test_tensor_general.py::test_bmm[cuda]\n",
            "  /usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 64 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "    warn(NumbaPerformanceWarning(msg))\n",
            "\n",
            "tests/test_tensor_general.py::test_bmm[cuda]\n",
            "  /usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 24 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "    warn(NumbaPerformanceWarning(msg))\n",
            "\n",
            "tests/test_tensor_general.py::test_bmm[cuda]\n",
            "  /usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 36 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "    warn(NumbaPerformanceWarning(msg))\n",
            "\n",
            "tests/test_tensor_general.py::test_bmm[cuda]\n",
            "  /usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 5 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "    warn(NumbaPerformanceWarning(msg))\n",
            "\n",
            "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
            "\u001b[33m========================== \u001b[32m117 passed\u001b[0m, \u001b[33m\u001b[1m4437 warnings\u001b[0m\u001b[33m in 386.34s (0:06:26)\u001b[0m\u001b[33m ==========================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "import minitorch\n",
        "import time\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "FastTensorBackend = minitorch.TensorBackend(minitorch.FastOps)\n",
        "GPUBackend = minitorch.TensorBackend(minitorch.CudaOps)\n",
        "\n",
        "def run_matmul(backend, size=16) -> None:\n",
        "    batch_size = 2\n",
        "\n",
        "    x = minitorch.rand((batch_size, size, size), backend=backend)\n",
        "    y = minitorch.rand((batch_size, size, size), backend=backend)\n",
        "    z = x @ y\n",
        "\n",
        "\n",
        "# Warmup\n",
        "run_matmul(FastTensorBackend)\n",
        "run_matmul(GPUBackend)\n",
        "\n",
        "ntrials = 3\n",
        "times = {}\n",
        "for size in [64, 128, 256, 512, 1024]:\n",
        "    print(f\"Running size {size}\")\n",
        "    times[size] = {}\n",
        "    simple_times = []\n",
        "    fast_times = []\n",
        "    gpu_times = []\n",
        "    for _ in range(ntrials):\n",
        "        start_fast = time.time()\n",
        "        run_matmul(FastTensorBackend, size)\n",
        "        end_fast = time.time()\n",
        "\n",
        "        start_gpu = time.time()\n",
        "        run_matmul(GPUBackend, size)\n",
        "        end_gpu = time.time()\n",
        "\n",
        "        fast_time = end_fast - start_fast\n",
        "        gpu_time = end_gpu - start_gpu\n",
        "\n",
        "        fast_times.append(fast_time)\n",
        "        gpu_times.append(gpu_time)\n",
        "\n",
        "    times[size][\"fast\"] = np.mean(fast_times)\n",
        "    times[size][\"gpu\"] = np.mean(gpu_times)\n",
        "    print(times[size])\n",
        "\n",
        "print()\n",
        "print(\"Timing summary\")\n",
        "for size, stimes in times.items():\n",
        "    print(f\"Size: {size}\")\n",
        "    for b, t in stimes.items():\n",
        "        print(f\"    {b}: {t:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFg5whxt2Kbm",
        "outputId": "ebd3ab3d-66d8-4580-8782-f6b95af11258"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 8 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 32 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running size 64\n",
            "{'fast': 0.0037082036336263022, 'gpu': 0.006421089172363281}\n",
            "Running size 128\n",
            "{'fast': 0.01814103126525879, 'gpu': 0.01659369468688965}\n",
            "Running size 256\n",
            "{'fast': 0.0999757448832194, 'gpu': 0.054840087890625}\n",
            "Running size 512\n",
            "{'fast': 0.9903890291849772, 'gpu': 0.2164646784464518}\n",
            "Running size 1024\n",
            "{'fast': 9.054831663767496, 'gpu': 1.175438404083252}\n",
            "\n",
            "Timing summary\n",
            "Size: 64\n",
            "    fast: 0.00371\n",
            "    gpu: 0.00642\n",
            "Size: 128\n",
            "    fast: 0.01814\n",
            "    gpu: 0.01659\n",
            "Size: 256\n",
            "    fast: 0.09998\n",
            "    gpu: 0.05484\n",
            "Size: 512\n",
            "    fast: 0.99039\n",
            "    gpu: 0.21646\n",
            "Size: 1024\n",
            "    fast: 9.05483\n",
            "    gpu: 1.17544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data from the provided results\n",
        "sizes = [64, 128, 256, 512, 1024]  # Sizes of the square matrices\n",
        "fast_times = [0.00371, 0.01814, 0.09998, 0.99039, 9.05483]  # Execution times for FastOps (naive Numba JIT)\n",
        "gpu_times = [0.00642, 0.01659, 0.05484, 0.21646, 1.17544]  # Execution times for CudaOps (CUDA implementation)\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(sizes, fast_times, label='FastOps (Naive Numba JIT)', marker='o', color='blue')\n",
        "plt.plot(sizes, gpu_times, label='CudaOps (CUDA Implementation)', marker='x', color='red')\n",
        "plt.xlabel('Matrix Size (n x n)')\n",
        "plt.ylabel('Execution Time (s)')\n",
        "plt.title('Comparison of Matrix Multiplication Execution Time')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "kXcqzKVf1Ufb",
        "outputId": "b9734a76-3689-4fc3-b62e-b6d68591d898"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Split"
      ],
      "metadata": {
        "id": "gfqIlWlMRkVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd $DIR; PYTHONPATH=/content/$DIR python3 project/run_fast_tensor.py --BACKEND cpu --HIDDEN 100 --DATASET split --RATE 0.05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIihdGspFbxh",
        "outputId": "7e289548-0cfa-4799-fd45-4a12d90f720c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 \t loss: 7.416926056050638 \t correct: 35 \t time: 1.9763298988342286\n",
            "Epoch: 10 \t loss: 6.792116212496707 \t correct: 41 \t time: 0.10316529273986816\n",
            "Epoch: 20 \t loss: 6.4929720284275225 \t correct: 47 \t time: 0.16965119838714598\n",
            "Epoch: 30 \t loss: 3.7119380141974654 \t correct: 46 \t time: 0.2241373062133789\n",
            "Epoch: 40 \t loss: 3.256689495337322 \t correct: 47 \t time: 0.22992217540740967\n",
            "Epoch: 50 \t loss: 2.1047522762097364 \t correct: 49 \t time: 0.17312679290771485\n",
            "Epoch: 60 \t loss: 1.630234171181419 \t correct: 49 \t time: 0.10134143829345703\n",
            "Epoch: 70 \t loss: 2.5691213706718394 \t correct: 49 \t time: 0.10219261646270753\n",
            "Epoch: 80 \t loss: 1.193947084999619 \t correct: 49 \t time: 0.10293803215026856\n",
            "Epoch: 90 \t loss: 1.2459348879919214 \t correct: 49 \t time: 0.13014256954193115\n",
            "Epoch: 100 \t loss: 1.5854711851616754 \t correct: 49 \t time: 0.1010373592376709\n",
            "Epoch: 110 \t loss: 1.200290023441873 \t correct: 49 \t time: 0.10425686836242676\n",
            "Epoch: 120 \t loss: 0.7314045505964265 \t correct: 49 \t time: 0.10081908702850342\n",
            "Epoch: 130 \t loss: 0.5970400240459084 \t correct: 49 \t time: 0.10023055076599122\n",
            "Epoch: 140 \t loss: 0.6955733792610108 \t correct: 49 \t time: 0.10091290473937989\n",
            "Epoch: 150 \t loss: 0.7057068198015551 \t correct: 49 \t time: 0.2039186954498291\n",
            "Epoch: 160 \t loss: 0.8696692454396773 \t correct: 49 \t time: 0.22645869255065917\n",
            "Epoch: 170 \t loss: 0.4801829385791019 \t correct: 49 \t time: 0.22891623973846437\n",
            "Epoch: 180 \t loss: 0.4699322579727792 \t correct: 49 \t time: 0.13071072101593018\n",
            "Epoch: 190 \t loss: 0.27240836506372706 \t correct: 49 \t time: 0.10251779556274414\n",
            "Epoch: 200 \t loss: 0.11697064698724144 \t correct: 49 \t time: 0.10057127475738525\n",
            "Epoch: 210 \t loss: 1.25964004850645 \t correct: 50 \t time: 0.10149497985839843\n",
            "Epoch: 220 \t loss: 0.18592621872918302 \t correct: 50 \t time: 0.10133697986602783\n",
            "Epoch: 230 \t loss: 0.3584523287653319 \t correct: 49 \t time: 0.09877607822418213\n",
            "Epoch: 240 \t loss: 1.1857987741611855 \t correct: 50 \t time: 0.09900994300842285\n",
            "Epoch: 250 \t loss: 0.4655290067784311 \t correct: 49 \t time: 0.10262494087219239\n",
            "Epoch: 260 \t loss: 0.08755238353906422 \t correct: 50 \t time: 0.10716710090637208\n",
            "Epoch: 270 \t loss: 1.0673053171754234 \t correct: 50 \t time: 0.11750800609588623\n",
            "Epoch: 280 \t loss: 0.22282054972684023 \t correct: 49 \t time: 0.2324007272720337\n",
            "Epoch: 290 \t loss: 0.054614562042594274 \t correct: 50 \t time: 0.1971668243408203\n",
            "Epoch: 300 \t loss: 0.3678509209031379 \t correct: 50 \t time: 0.22619938850402832\n",
            "Epoch: 310 \t loss: 1.1213041951901483 \t correct: 50 \t time: 0.12219753265380859\n",
            "Epoch: 320 \t loss: 0.12574386520774747 \t correct: 50 \t time: 0.09924209117889404\n",
            "Epoch: 330 \t loss: 0.4037794613442156 \t correct: 50 \t time: 0.10246634483337402\n",
            "Epoch: 340 \t loss: 0.18387471119156074 \t correct: 50 \t time: 0.10014817714691163\n",
            "Epoch: 350 \t loss: 0.24592526835095554 \t correct: 50 \t time: 0.10341775417327881\n",
            "Epoch: 360 \t loss: 0.05926813039279591 \t correct: 50 \t time: 0.09846253395080566\n",
            "Epoch: 370 \t loss: 0.23027514742565364 \t correct: 50 \t time: 0.09795236587524414\n",
            "Epoch: 380 \t loss: 0.09090080988521469 \t correct: 50 \t time: 0.09912915229797363\n",
            "Epoch: 390 \t loss: 0.2183102512124395 \t correct: 50 \t time: 0.10087974071502685\n",
            "Epoch: 400 \t loss: 0.5609351291812226 \t correct: 50 \t time: 0.10073714256286621\n",
            "Epoch: 410 \t loss: 0.27468009996653675 \t correct: 50 \t time: 0.2067584991455078\n",
            "Epoch: 420 \t loss: 0.11189263243316709 \t correct: 50 \t time: 0.20749413967132568\n",
            "Epoch: 430 \t loss: 0.09213631842018738 \t correct: 50 \t time: 0.22740023136138915\n",
            "Epoch: 440 \t loss: 0.07826507554140598 \t correct: 50 \t time: 0.14264404773712158\n",
            "Epoch: 450 \t loss: 0.19168719564876982 \t correct: 50 \t time: 0.10887742042541504\n",
            "Epoch: 460 \t loss: 0.0201110057714424 \t correct: 50 \t time: 0.10987112522125245\n",
            "Epoch: 470 \t loss: 0.10063141712320638 \t correct: 50 \t time: 0.11324777603149414\n",
            "Epoch: 480 \t loss: 0.5197503928592335 \t correct: 50 \t time: 0.12039060592651367\n",
            "Epoch: 490 \t loss: 0.48899427388894484 \t correct: 50 \t time: 0.13115036487579346\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FY8Q4MqEzvGW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c79974a6-324d-4ce8-e707-6191399786cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 32 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 32 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 32 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 100 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 16 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 8 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "Epoch: 0 \t loss: 8.023539736529125 \t correct: 38 \t time: 0.6759792566299438\n",
            "Epoch: 10 \t loss: 5.5821856058737716 \t correct: 41 \t time: 2.147471475601196\n",
            "Epoch: 20 \t loss: 3.24271591847363 \t correct: 41 \t time: 2.1366048097610473\n",
            "Epoch: 30 \t loss: 5.084539983675283 \t correct: 45 \t time: 2.319426488876343\n",
            "Epoch: 40 \t loss: 3.1265014520912935 \t correct: 46 \t time: 2.10177161693573\n",
            "Epoch: 50 \t loss: 3.9494815744044818 \t correct: 46 \t time: 2.119654130935669\n",
            "Epoch: 60 \t loss: 3.065202189397376 \t correct: 46 \t time: 2.2724744081497192\n",
            "Epoch: 70 \t loss: 3.7796835289728286 \t correct: 47 \t time: 2.1811086893081666\n",
            "Epoch: 80 \t loss: 2.5759446332705696 \t correct: 42 \t time: 2.1379848957061767\n",
            "Epoch: 90 \t loss: 3.1021224017854423 \t correct: 47 \t time: 2.1649187564849854\n",
            "Epoch: 100 \t loss: 1.1487212920883538 \t correct: 49 \t time: 2.2820067167282105\n",
            "Epoch: 110 \t loss: 1.6943231482261547 \t correct: 49 \t time: 2.08520884513855\n",
            "Epoch: 120 \t loss: 1.373993585811249 \t correct: 48 \t time: 2.0874931812286377\n",
            "Epoch: 130 \t loss: 0.9072663873948379 \t correct: 48 \t time: 2.2722702264785766\n",
            "Epoch: 140 \t loss: 0.8153935215383017 \t correct: 49 \t time: 2.173226261138916\n",
            "Epoch: 150 \t loss: 1.362351509766932 \t correct: 49 \t time: 2.1036305904388426\n",
            "Epoch: 160 \t loss: 1.3469583373711342 \t correct: 47 \t time: 2.208749556541443\n",
            "Epoch: 170 \t loss: 1.0017002102518462 \t correct: 49 \t time: 2.256648564338684\n",
            "Epoch: 180 \t loss: 0.9957496439826833 \t correct: 49 \t time: 2.103678798675537\n",
            "Epoch: 190 \t loss: 0.639693828695966 \t correct: 49 \t time: 2.1098540782928468\n",
            "Epoch: 200 \t loss: 0.5354974070674015 \t correct: 49 \t time: 2.3549691915512083\n",
            "Epoch: 210 \t loss: 1.0644956957226726 \t correct: 49 \t time: 2.1133179903030395\n",
            "Epoch: 220 \t loss: 0.7215675738621975 \t correct: 49 \t time: 2.093727540969849\n",
            "Epoch: 230 \t loss: 2.5992049638814505 \t correct: 49 \t time: 2.260373568534851\n",
            "Epoch: 240 \t loss: 0.23289148422173456 \t correct: 49 \t time: 2.1937474966049195\n",
            "Epoch: 250 \t loss: 0.9122723562513666 \t correct: 50 \t time: 2.1287903070449827\n",
            "Epoch: 260 \t loss: 0.1934690924744426 \t correct: 50 \t time: 2.1582913160324098\n",
            "Epoch: 270 \t loss: 0.6017984631712474 \t correct: 49 \t time: 2.3353777647018434\n",
            "Epoch: 280 \t loss: 0.9070067379473788 \t correct: 49 \t time: 2.1175817489624023\n",
            "Epoch: 290 \t loss: 0.08143053280782347 \t correct: 50 \t time: 2.10555214881897\n",
            "Epoch: 300 \t loss: 0.8896901351229632 \t correct: 49 \t time: 2.3228121757507325\n",
            "Epoch: 310 \t loss: 0.9794970906699882 \t correct: 49 \t time: 2.1547027826309204\n",
            "Epoch: 320 \t loss: 0.6092274162695305 \t correct: 49 \t time: 2.1036127328872682\n",
            "Epoch: 330 \t loss: 1.1263705388470788 \t correct: 50 \t time: 2.262516140937805\n",
            "Epoch: 340 \t loss: 1.3092362699806126 \t correct: 50 \t time: 2.284215211868286\n",
            "Epoch: 350 \t loss: 0.15586870201030856 \t correct: 50 \t time: 2.096510481834412\n",
            "Epoch: 360 \t loss: 0.3921405190783464 \t correct: 50 \t time: 2.126324677467346\n",
            "Epoch: 370 \t loss: 1.3857272155351494 \t correct: 50 \t time: 2.3333044767379763\n",
            "Epoch: 380 \t loss: 0.5754365214473582 \t correct: 50 \t time: 2.089085030555725\n",
            "Epoch: 390 \t loss: 0.4123086344549868 \t correct: 49 \t time: 2.088583254814148\n",
            "Epoch: 400 \t loss: 0.6436248027273457 \t correct: 50 \t time: 2.2217939853668214\n",
            "Epoch: 410 \t loss: 0.2662424687906599 \t correct: 50 \t time: 2.2337647676467896\n",
            "Epoch: 420 \t loss: 0.633545295307098 \t correct: 50 \t time: 2.142848801612854\n",
            "Epoch: 430 \t loss: 0.8396489614671401 \t correct: 49 \t time: 2.1305474281311034\n",
            "Epoch: 440 \t loss: 0.31524076982844784 \t correct: 50 \t time: 2.321561026573181\n",
            "Epoch: 450 \t loss: 0.06618714041500091 \t correct: 50 \t time: 2.1034682989120483\n",
            "Epoch: 460 \t loss: 0.24161712710131597 \t correct: 50 \t time: 2.114767241477966\n",
            "Epoch: 470 \t loss: 0.7328657058858211 \t correct: 49 \t time: 2.276424503326416\n",
            "Epoch: 480 \t loss: 0.4642621708578311 \t correct: 50 \t time: 2.1753551244735716\n",
            "Epoch: 490 \t loss: 0.37944184749431503 \t correct: 50 \t time: 2.1051730871200562\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!cd $DIR; PYTHONPATH=/content/$DIR python3 project/run_fast_tensor.py --BACKEND gpu --HIDDEN 100 --DATASET split --RATE 0.05"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Simple"
      ],
      "metadata": {
        "id": "O3WGTjP_Rf8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd $DIR; PYTHONPATH=/content/$DIR python3 project/run_fast_tensor.py --BACKEND gpu --HIDDEN 100 --DATASET simple --RATE 0.05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gx6kr3mYHHth",
        "outputId": "01a53eb0-f492-4d5a-e3ab-0103a04a67ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 32 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 32 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 32 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 100 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 16 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 8 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "Epoch: 0 \t loss: 5.55330612977303 \t correct: 43 \t time: 0.4966816663742065\n",
            "Epoch: 10 \t loss: 1.9957080568233119 \t correct: 49 \t time: 2.284906435012817\n",
            "Epoch: 20 \t loss: 1.1322363570629197 \t correct: 50 \t time: 2.183769774436951\n",
            "Epoch: 30 \t loss: 0.6266136642881713 \t correct: 50 \t time: 2.1000087738037108\n",
            "Epoch: 40 \t loss: 1.066012619798161 \t correct: 50 \t time: 2.186567711830139\n",
            "Epoch: 50 \t loss: 0.6178279107445204 \t correct: 50 \t time: 2.267444610595703\n",
            "Epoch: 60 \t loss: 1.1922132532713756 \t correct: 50 \t time: 2.1220214128494264\n",
            "Epoch: 70 \t loss: 0.7234757980676024 \t correct: 50 \t time: 2.125244402885437\n",
            "Epoch: 80 \t loss: 0.4848284441308814 \t correct: 50 \t time: 2.3540323972702026\n",
            "Epoch: 90 \t loss: 1.0696902373432318 \t correct: 50 \t time: 2.10403094291687\n",
            "Epoch: 100 \t loss: 0.6963811751107061 \t correct: 50 \t time: 2.0992087841033937\n",
            "Epoch: 110 \t loss: 0.5641653670009793 \t correct: 50 \t time: 2.2827664375305177\n",
            "Epoch: 120 \t loss: 0.21715948983400174 \t correct: 50 \t time: 2.1724411725997923\n",
            "Epoch: 130 \t loss: 0.6602503314074103 \t correct: 50 \t time: 2.112850880622864\n",
            "Epoch: 140 \t loss: 0.4007584431252436 \t correct: 50 \t time: 2.2045236110687254\n",
            "Epoch: 150 \t loss: 0.1626016259501473 \t correct: 50 \t time: 2.29017231464386\n",
            "Epoch: 160 \t loss: 0.09769975043873828 \t correct: 50 \t time: 2.0848377466201784\n",
            "Epoch: 170 \t loss: 0.43804454700467516 \t correct: 50 \t time: 2.11133189201355\n",
            "Epoch: 180 \t loss: 0.397904650448048 \t correct: 50 \t time: 2.3488156318664553\n",
            "Epoch: 190 \t loss: 0.23936566650920627 \t correct: 50 \t time: 2.1022849321365356\n",
            "Epoch: 200 \t loss: 0.36630936795212793 \t correct: 50 \t time: 2.1079967975616456\n",
            "Epoch: 210 \t loss: 0.027689861352395265 \t correct: 50 \t time: 2.2843506574630736\n",
            "Epoch: 220 \t loss: 0.3245106051474864 \t correct: 50 \t time: 2.1907992362976074\n",
            "Epoch: 230 \t loss: 0.06531932303837217 \t correct: 50 \t time: 2.11426682472229\n",
            "Epoch: 240 \t loss: 0.16125631323054876 \t correct: 50 \t time: 2.180779814720154\n",
            "Epoch: 250 \t loss: 0.07842262616919186 \t correct: 50 \t time: 2.2706786394119263\n",
            "Epoch: 260 \t loss: 0.08489400971933456 \t correct: 50 \t time: 2.1145328521728515\n",
            "Epoch: 270 \t loss: 0.3104198776294714 \t correct: 50 \t time: 2.0837549924850465\n",
            "Epoch: 280 \t loss: 0.05364555526518152 \t correct: 50 \t time: 2.360784578323364\n",
            "Epoch: 290 \t loss: 0.3169184543404541 \t correct: 50 \t time: 2.1024883508682253\n",
            "Epoch: 300 \t loss: 0.017137195488231904 \t correct: 50 \t time: 2.0988242864608764\n",
            "Epoch: 310 \t loss: 0.07839321806537117 \t correct: 50 \t time: 2.2391273975372314\n",
            "Epoch: 320 \t loss: 0.05935719547217156 \t correct: 50 \t time: 2.272798705101013\n",
            "Epoch: 330 \t loss: 0.0874024957629584 \t correct: 50 \t time: 2.139209747314453\n",
            "Epoch: 340 \t loss: 0.0004443077940283366 \t correct: 50 \t time: 2.2091679096221926\n",
            "Epoch: 350 \t loss: 0.09008095752639722 \t correct: 50 \t time: 2.3182801723480226\n",
            "Epoch: 360 \t loss: 0.06422344370398576 \t correct: 50 \t time: 2.1057631731033326\n",
            "Epoch: 370 \t loss: 0.10020771475977826 \t correct: 50 \t time: 2.1233279705047607\n",
            "Epoch: 380 \t loss: 0.018387464349437273 \t correct: 50 \t time: 2.328896474838257\n",
            "Epoch: 390 \t loss: 0.31018563876582395 \t correct: 50 \t time: 2.096540665626526\n",
            "Epoch: 400 \t loss: 0.05410240727040144 \t correct: 50 \t time: 2.1121410131454468\n",
            "Epoch: 410 \t loss: 0.11033983137818748 \t correct: 50 \t time: 2.271157908439636\n",
            "Epoch: 420 \t loss: 0.0309633452649768 \t correct: 50 \t time: 2.1778453588485718\n",
            "Epoch: 430 \t loss: 0.03179979919513687 \t correct: 50 \t time: 2.0944753885269165\n",
            "Epoch: 440 \t loss: 0.159156514066438 \t correct: 50 \t time: 2.1814504623413087\n",
            "Epoch: 450 \t loss: 0.18584295340442156 \t correct: 50 \t time: 2.2916350603103637\n",
            "Epoch: 460 \t loss: 0.016454020263133355 \t correct: 50 \t time: 2.0997920513153074\n",
            "Epoch: 470 \t loss: 0.04259274993844469 \t correct: 50 \t time: 2.098102355003357\n",
            "Epoch: 480 \t loss: 0.005217807061522903 \t correct: 50 \t time: 2.315219855308533\n",
            "Epoch: 490 \t loss: 1.5152159428219971e-05 \t correct: 50 \t time: 2.154339551925659\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Xor"
      ],
      "metadata": {
        "id": "c63ENX50RuTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd $DIR; PYTHONPATH=/content/$DIR python3 project/run_fast_tensor.py --BACKEND gpu --HIDDEN 100 --DATASET xor --RATE 0.05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-JNwdzROSkO",
        "outputId": "a7b88cce-2264-4270-a9ba-1076381428e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 32 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 32 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 32 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 100 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 16 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 8 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "Epoch: 0 \t loss: 7.194664995685286 \t correct: 32 \t time: 0.39042646884918214\n",
            "Epoch: 10 \t loss: 5.518670173316256 \t correct: 42 \t time: 2.202633333206177\n",
            "Epoch: 20 \t loss: 4.136710621285703 \t correct: 43 \t time: 2.3226151704788207\n",
            "Epoch: 30 \t loss: 4.073732083131506 \t correct: 42 \t time: 2.13520188331604\n",
            "Epoch: 40 \t loss: 4.266792865630286 \t correct: 45 \t time: 2.1470532178878785\n",
            "Epoch: 50 \t loss: 2.8447455969344295 \t correct: 44 \t time: 2.352779769897461\n",
            "Epoch: 60 \t loss: 1.8396069141452536 \t correct: 46 \t time: 2.145019268989563\n",
            "Epoch: 70 \t loss: 3.849007885304477 \t correct: 46 \t time: 2.139802026748657\n",
            "Epoch: 80 \t loss: 2.080620047920248 \t correct: 46 \t time: 2.385815453529358\n",
            "Epoch: 90 \t loss: 3.521374085835361 \t correct: 47 \t time: 2.1322012662887575\n",
            "Epoch: 100 \t loss: 1.6718151110893043 \t correct: 46 \t time: 2.125269818305969\n",
            "Epoch: 110 \t loss: 2.2011520445700117 \t correct: 44 \t time: 2.3266170740127565\n",
            "Epoch: 120 \t loss: 1.0769532848295027 \t correct: 45 \t time: 2.1943318843841553\n",
            "Epoch: 130 \t loss: 2.78170929396493 \t correct: 47 \t time: 2.1174461126327513\n",
            "Epoch: 140 \t loss: 2.6148032740514267 \t correct: 47 \t time: 2.2872160911560058\n",
            "Epoch: 150 \t loss: 2.207618838829578 \t correct: 46 \t time: 2.247498869895935\n",
            "Epoch: 160 \t loss: 1.2657368445326416 \t correct: 48 \t time: 2.130723738670349\n",
            "Epoch: 170 \t loss: 0.6176094467194196 \t correct: 48 \t time: 2.2153765201568603\n",
            "Epoch: 180 \t loss: 1.0314022701621783 \t correct: 49 \t time: 2.2945773601531982\n",
            "Epoch: 190 \t loss: 1.2472845981010126 \t correct: 48 \t time: 2.1435618877410887\n",
            "Epoch: 200 \t loss: 0.9680186665268209 \t correct: 45 \t time: 2.203331732749939\n",
            "Epoch: 210 \t loss: 1.050912569817421 \t correct: 46 \t time: 2.3200711011886597\n",
            "Epoch: 220 \t loss: 2.267320677123325 \t correct: 47 \t time: 2.122454619407654\n",
            "Epoch: 230 \t loss: 3.5781419432878416 \t correct: 49 \t time: 2.166304898262024\n",
            "Epoch: 240 \t loss: 0.5840607403150286 \t correct: 49 \t time: 2.3481820583343507\n",
            "Epoch: 250 \t loss: 1.9571346293138634 \t correct: 47 \t time: 2.115333008766174\n",
            "Epoch: 260 \t loss: 0.3226803124594151 \t correct: 49 \t time: 2.134294557571411\n",
            "Epoch: 270 \t loss: 0.8950017890327286 \t correct: 46 \t time: 2.348386311531067\n",
            "Epoch: 280 \t loss: 1.8412122126779944 \t correct: 49 \t time: 2.1718087911605837\n",
            "Epoch: 290 \t loss: 0.5107177231770013 \t correct: 47 \t time: 2.132834792137146\n",
            "Epoch: 300 \t loss: 0.09907781364468794 \t correct: 50 \t time: 2.2898624658584597\n",
            "Epoch: 310 \t loss: 0.9166146355445961 \t correct: 48 \t time: 2.195593738555908\n",
            "Epoch: 320 \t loss: 1.6094050441549332 \t correct: 49 \t time: 2.1439659118652346\n",
            "Epoch: 330 \t loss: 2.152595831048399 \t correct: 49 \t time: 2.2068852186203003\n",
            "Epoch: 340 \t loss: 0.8818154897767102 \t correct: 48 \t time: 2.26354124546051\n",
            "Epoch: 350 \t loss: 0.7667879688112351 \t correct: 49 \t time: 2.132659888267517\n",
            "Epoch: 360 \t loss: 0.6370065745980709 \t correct: 49 \t time: 2.211960530281067\n",
            "Epoch: 370 \t loss: 0.5819790056858314 \t correct: 49 \t time: 2.282002878189087\n",
            "Epoch: 380 \t loss: 0.3805509887252293 \t correct: 49 \t time: 2.119944930076599\n",
            "Epoch: 390 \t loss: 2.2834867051436962 \t correct: 50 \t time: 2.1692577362060548\n",
            "Epoch: 400 \t loss: 0.35566142905631 \t correct: 48 \t time: 2.350715756416321\n",
            "Epoch: 410 \t loss: 1.2100380672180306 \t correct: 50 \t time: 2.165178990364075\n",
            "Epoch: 420 \t loss: 0.879569197691435 \t correct: 50 \t time: 2.1287463426589968\n",
            "Epoch: 430 \t loss: 1.836880243596537 \t correct: 48 \t time: 2.3750969409942626\n",
            "Epoch: 440 \t loss: 1.0847649383197597 \t correct: 49 \t time: 2.1370877981185914\n",
            "Epoch: 450 \t loss: 1.42867351682977 \t correct: 48 \t time: 2.1118388175964355\n",
            "Epoch: 460 \t loss: 2.2278686191059687 \t correct: 49 \t time: 2.326609492301941\n",
            "Epoch: 470 \t loss: 0.6101836511747742 \t correct: 49 \t time: 2.1978176832199097\n",
            "Epoch: 480 \t loss: 0.7120380564857185 \t correct: 50 \t time: 2.107789659500122\n",
            "Epoch: 490 \t loss: 0.11533232890149278 \t correct: 49 \t time: 2.2336391687393187\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bigger Model"
      ],
      "metadata": {
        "id": "i-NO1w9zYuq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd $DIR; PYTHONPATH=/content/$DIR python3 project/run_fast_tensor.py --BACKEND cpu --HIDDEN 200 --DATASET xor --RATE 0.05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_GHC_WDT9mP",
        "outputId": "d5f73eeb-26e5-4eab-e3d6-773ec41bf036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 \t loss: 26.46205136491111 \t correct: 30 \t time: 2.0600966691970823\n",
            "Epoch: 10 \t loss: 4.234059050136677 \t correct: 40 \t time: 0.18702781200408936\n",
            "Epoch: 20 \t loss: 2.2847054809279017 \t correct: 45 \t time: 0.18785955905914306\n",
            "Epoch: 30 \t loss: 2.8511816761244537 \t correct: 45 \t time: 0.18684537410736085\n",
            "Epoch: 40 \t loss: 0.40831809451215706 \t correct: 46 \t time: 0.1888136386871338\n",
            "Epoch: 50 \t loss: 2.7538642110563645 \t correct: 47 \t time: 0.32138915061950685\n",
            "Epoch: 60 \t loss: 2.504938147400266 \t correct: 46 \t time: 0.36296184062957765\n",
            "Epoch: 70 \t loss: 1.1421058341588832 \t correct: 47 \t time: 0.22458093166351317\n",
            "Epoch: 80 \t loss: 2.0368028953090853 \t correct: 47 \t time: 0.18949601650238038\n",
            "Epoch: 90 \t loss: 1.001511398240223 \t correct: 49 \t time: 0.1904442071914673\n",
            "Epoch: 100 \t loss: 0.8466210528373413 \t correct: 47 \t time: 0.18654510974884034\n",
            "Epoch: 110 \t loss: 0.7135480528568742 \t correct: 49 \t time: 0.18686647415161134\n",
            "Epoch: 120 \t loss: 2.0869605020491315 \t correct: 49 \t time: 0.27227420806884767\n",
            "Epoch: 130 \t loss: 2.260253514747112 \t correct: 49 \t time: 0.364253306388855\n",
            "Epoch: 140 \t loss: 1.0205851474780798 \t correct: 47 \t time: 0.276145339012146\n",
            "Epoch: 150 \t loss: 0.8087359768138471 \t correct: 48 \t time: 0.1863924741744995\n",
            "Epoch: 160 \t loss: 0.7645088794317746 \t correct: 50 \t time: 0.187701416015625\n",
            "Epoch: 170 \t loss: 1.5640728746897619 \t correct: 48 \t time: 0.18479361534118652\n",
            "Epoch: 180 \t loss: 0.5327983074315565 \t correct: 50 \t time: 0.1974231243133545\n",
            "Epoch: 190 \t loss: 0.16970034391890787 \t correct: 46 \t time: 0.25103116035461426\n",
            "Epoch: 200 \t loss: 1.2176687523630794 \t correct: 50 \t time: 0.35154478549957274\n",
            "Epoch: 210 \t loss: 1.8871337229321719 \t correct: 50 \t time: 0.3205507755279541\n",
            "Epoch: 220 \t loss: 0.9992704363672694 \t correct: 50 \t time: 0.18679273128509521\n",
            "Epoch: 230 \t loss: 1.2090863721155198 \t correct: 50 \t time: 0.18725650310516356\n",
            "Epoch: 240 \t loss: 1.7870997232905161 \t correct: 50 \t time: 0.1835240125656128\n",
            "Epoch: 250 \t loss: 1.3344969465387806 \t correct: 47 \t time: 0.1886823892593384\n",
            "Epoch: 260 \t loss: 0.669648179418256 \t correct: 50 \t time: 0.1963355779647827\n",
            "Epoch: 270 \t loss: 0.5001524588403417 \t correct: 50 \t time: 0.3491281270980835\n",
            "Epoch: 280 \t loss: 0.34212872148460727 \t correct: 50 \t time: 0.3676774024963379\n",
            "Epoch: 290 \t loss: 0.409908137757465 \t correct: 49 \t time: 0.1887042760848999\n",
            "Epoch: 300 \t loss: 0.9694275162400267 \t correct: 50 \t time: 0.19261059761047364\n",
            "Epoch: 310 \t loss: 0.05724592962299196 \t correct: 50 \t time: 0.1894397735595703\n",
            "Epoch: 320 \t loss: 0.2729113880827773 \t correct: 50 \t time: 0.18732409477233886\n",
            "Epoch: 330 \t loss: 1.6458732044764384 \t correct: 48 \t time: 0.18783435821533204\n",
            "Epoch: 340 \t loss: 0.3671959702043178 \t correct: 50 \t time: 0.35276901721954346\n",
            "Epoch: 350 \t loss: 1.543290797625887 \t correct: 49 \t time: 0.35841569900512693\n",
            "Epoch: 360 \t loss: 0.955262946769746 \t correct: 50 \t time: 0.18977525234222412\n",
            "Epoch: 370 \t loss: 1.18646128451159 \t correct: 50 \t time: 0.18546659946441652\n",
            "Epoch: 380 \t loss: 0.573254335089028 \t correct: 50 \t time: 0.19106452465057372\n",
            "Epoch: 390 \t loss: 1.1716226463903956 \t correct: 50 \t time: 0.18438079357147216\n",
            "Epoch: 400 \t loss: 0.3889801019984124 \t correct: 50 \t time: 0.18534629344940184\n",
            "Epoch: 410 \t loss: 0.04330528406296372 \t correct: 50 \t time: 0.2849681615829468\n",
            "Epoch: 420 \t loss: 0.16680464977946002 \t correct: 50 \t time: 0.3651202440261841\n",
            "Epoch: 430 \t loss: 0.22760731823224048 \t correct: 50 \t time: 0.26432464122772215\n",
            "Epoch: 440 \t loss: 0.2744749428981803 \t correct: 50 \t time: 0.1889045476913452\n",
            "Epoch: 450 \t loss: 0.8795007832099508 \t correct: 50 \t time: 0.18470416069030762\n",
            "Epoch: 460 \t loss: 0.7419510571897566 \t correct: 50 \t time: 0.1860586643218994\n",
            "Epoch: 470 \t loss: 0.31485630599453357 \t correct: 50 \t time: 0.18295869827270508\n",
            "Epoch: 480 \t loss: 0.13372929801389163 \t correct: 49 \t time: 0.21827976703643798\n",
            "Epoch: 490 \t loss: 0.0973941075868659 \t correct: 50 \t time: 0.355080246925354\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd $DIR; PYTHONPATH=/content/$DIR python3 project/run_fast_tensor.py --BACKEND gpu --HIDDEN 200 --DATASET xor --RATE 0.05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5K9whjAxY0wy",
        "outputId": "947f3f1d-ffcb-4d6e-dc55-9ce8198c4940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 13 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 13 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 63 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 63 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 63 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 49 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 13 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 7 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 14 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 2 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "Epoch: 0 \t loss: 4.046003511893338 \t correct: 30 \t time: 0.4230589628219604\n",
            "Epoch: 10 \t loss: 1.3181062550627933 \t correct: 41 \t time: 2.3893728256225586\n",
            "Epoch: 20 \t loss: 1.7866191498816137 \t correct: 44 \t time: 2.1804355144500733\n",
            "Epoch: 30 \t loss: 2.794972373211081 \t correct: 49 \t time: 2.327899718284607\n",
            "Epoch: 40 \t loss: 1.9541289007368823 \t correct: 47 \t time: 2.332233190536499\n",
            "Epoch: 50 \t loss: 1.23438235267232 \t correct: 48 \t time: 2.217619037628174\n",
            "Epoch: 60 \t loss: 2.3387388542738896 \t correct: 47 \t time: 2.4683841705322265\n",
            "Epoch: 70 \t loss: 1.1981829737042804 \t correct: 49 \t time: 2.2237198114395142\n",
            "Epoch: 80 \t loss: 1.6025786686355419 \t correct: 49 \t time: 2.28967764377594\n",
            "Epoch: 90 \t loss: 0.7412814864608995 \t correct: 49 \t time: 2.449548602104187\n",
            "Epoch: 100 \t loss: 1.0100854449759302 \t correct: 49 \t time: 2.215818238258362\n",
            "Epoch: 110 \t loss: 4.738135919081292 \t correct: 49 \t time: 2.3566921234130858\n",
            "Epoch: 120 \t loss: 2.2598860251904687 \t correct: 48 \t time: 2.3360361814498902\n",
            "Epoch: 130 \t loss: 1.8007215355932895 \t correct: 49 \t time: 2.2298171281814576\n",
            "Epoch: 140 \t loss: 1.1620618246668748 \t correct: 49 \t time: 2.4876123666763306\n",
            "Epoch: 150 \t loss: 0.5210536228252447 \t correct: 49 \t time: 2.209350752830505\n",
            "Epoch: 160 \t loss: 1.2585367564891423 \t correct: 49 \t time: 2.2421059370040894\n",
            "Epoch: 170 \t loss: 2.1568580931483936 \t correct: 50 \t time: 2.462639331817627\n",
            "Epoch: 180 \t loss: 0.726748107286407 \t correct: 48 \t time: 2.21211416721344\n",
            "Epoch: 190 \t loss: 0.27266731744060285 \t correct: 50 \t time: 2.308778738975525\n",
            "Epoch: 200 \t loss: 0.4305888427711483 \t correct: 49 \t time: 2.362569546699524\n",
            "Epoch: 210 \t loss: 0.28837346643767137 \t correct: 50 \t time: 2.2063506841659546\n",
            "Epoch: 220 \t loss: 0.20495377696995715 \t correct: 49 \t time: 2.400188755989075\n",
            "Epoch: 230 \t loss: 0.05535149828562301 \t correct: 50 \t time: 2.2543551683425904\n",
            "Epoch: 240 \t loss: 0.3077334255122875 \t correct: 50 \t time: 2.204996109008789\n",
            "Epoch: 250 \t loss: 0.36463228682717 \t correct: 49 \t time: 2.4925485134124754\n",
            "Epoch: 260 \t loss: 0.1711592871066513 \t correct: 50 \t time: 2.208894968032837\n",
            "Epoch: 270 \t loss: 0.5255527359232237 \t correct: 50 \t time: 2.2500433921813965\n",
            "Epoch: 280 \t loss: 0.6218495185904184 \t correct: 50 \t time: 2.3960714817047117\n",
            "Epoch: 290 \t loss: 1.7858465164113948 \t correct: 50 \t time: 2.2268184661865233\n",
            "Epoch: 300 \t loss: 0.6491523313774682 \t correct: 49 \t time: 2.3594318151474\n",
            "Epoch: 310 \t loss: 0.12285829728504048 \t correct: 50 \t time: 2.3111589670181276\n",
            "Epoch: 320 \t loss: 0.4325539094380133 \t correct: 50 \t time: 2.214467167854309\n",
            "Epoch: 330 \t loss: 0.09456075070607999 \t correct: 50 \t time: 2.424709916114807\n",
            "Epoch: 340 \t loss: 0.38356474825328735 \t correct: 50 \t time: 2.239116883277893\n",
            "Epoch: 350 \t loss: 0.20460313840345493 \t correct: 50 \t time: 2.230786442756653\n",
            "Epoch: 360 \t loss: 0.3931084372900771 \t correct: 50 \t time: 2.437948751449585\n",
            "Epoch: 370 \t loss: 1.0893907480766305 \t correct: 50 \t time: 2.2010202407836914\n",
            "Epoch: 380 \t loss: 0.5766801204392629 \t correct: 50 \t time: 2.316675090789795\n",
            "Epoch: 390 \t loss: 0.3927667067356843 \t correct: 50 \t time: 2.2618538618087767\n",
            "Epoch: 400 \t loss: 0.3800373340733371 \t correct: 50 \t time: 2.2077431440353394\n",
            "Epoch: 410 \t loss: 0.15426868781501124 \t correct: 50 \t time: 2.315204930305481\n",
            "Epoch: 420 \t loss: 0.2232475774861185 \t correct: 50 \t time: 2.1573935985565185\n",
            "Epoch: 430 \t loss: 0.19725395037888346 \t correct: 50 \t time: 2.37328987121582\n",
            "Epoch: 440 \t loss: 0.5816087930494459 \t correct: 50 \t time: 2.1444195747375487\n",
            "Epoch: 450 \t loss: 0.21151691039650494 \t correct: 50 \t time: 2.361870050430298\n",
            "Epoch: 460 \t loss: 0.10536998979547058 \t correct: 50 \t time: 2.1577337980270386\n",
            "Epoch: 470 \t loss: 0.15410671283705854 \t correct: 50 \t time: 2.364146876335144\n",
            "Epoch: 480 \t loss: 0.29106941668242475 \t correct: 50 \t time: 2.136246109008789\n",
            "Epoch: 490 \t loss: 0.45173028212770416 \t correct: 50 \t time: 2.3403259038925173\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pWU-2_yQZsi5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}